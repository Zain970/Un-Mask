{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31808,"status":"ok","timestamp":1673167142358,"user":{"displayName":"I190538 Zain Ul Abidin","userId":"16664429988624671331"},"user_tz":-300},"id":"u2Q2-G2JX0q2","outputId":"91daf327-ad44-42be-cb4f-52d893014d85"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":32194,"status":"ok","timestamp":1673167178794,"user":{"displayName":"I190538 Zain Ul Abidin","userId":"16664429988624671331"},"user_tz":-300},"id":"3vj15Vde0Gxs","outputId":"0ced2ecf-3b98-4d61-854d-c89d74156733"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting face_recognition\n","  Downloading face_recognition-1.3.0-py2.py3-none-any.whl (15 kB)\n","Collecting face-recognition-models>=0.3.0\n","  Downloading face_recognition_models-0.3.0.tar.gz (100.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.1/100.1 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: dlib>=19.7 in /usr/local/lib/python3.8/dist-packages (from face_recognition) (19.24.0)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.8/dist-packages (from face_recognition) (7.1.2)\n","Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.8/dist-packages (from face_recognition) (7.1.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from face_recognition) (1.21.6)\n","Building wheels for collected packages: face-recognition-models\n","  Building wheel for face-recognition-models (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for face-recognition-models: filename=face_recognition_models-0.3.0-py2.py3-none-any.whl size=100566185 sha256=bd24a9e3411c1cd55c55fabec5463d9e8dcf00952f5c788e65bf6247fe0c72e1\n","  Stored in directory: /root/.cache/pip/wheels/b4/4b/8f/751e99d45f089bdf366a7d3e5066db3c2b84a62e4377f534d7\n","Successfully built face-recognition-models\n","Installing collected packages: face-recognition-models, face_recognition\n","Successfully installed face-recognition-models-0.3.0 face_recognition-1.3.0\n"]}],"source":["# Libraries\n","!pip3 install face_recognition\n","\n","import torch\n","import torchvision\n","from torchvision import transforms\n","from torch.utils.data import DataLoader\n","from torch.utils.data.dataset import Dataset\n","import os\n","import numpy as np\n","import cv2\n","import matplotlib.pyplot as plt\n","import face_recognition\n","\n","\n","import torch\n","from torch.autograd import Variable\n","import time\n","import os\n","import sys\n","import os\n","from torch import nn\n","from torchvision import models"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":16,"status":"ok","timestamp":1673167188679,"user":{"displayName":"I190538 Zain Ul Abidin","userId":"16664429988624671331"},"user_tz":-300},"id":"g1G0IoVy2Pc7"},"outputs":[],"source":["#Model with feature visualization\n","from torch import nn\n","from torchvision import models\n","class Model(nn.Module):\n","    def __init__(self, num_classes,latent_dim= 2048, lstm_layers=1 , hidden_dim = 2048, bidirectional = False):\n","\n","        super(Model, self).__init__()\n","\n","        # Feature\n","        model = models.resnext50_32x4d(pretrained = True)\n","\n","        self.model = nn.Sequential(*list(model.children())[:-2])\n","\n","        # Classification\n","        self.lstm = nn.LSTM(latent_dim,hidden_dim, lstm_layers,  bidirectional)\n","\n","        # Activation function\n","        self.relu = nn.LeakyReLU()\n","\n","        self.dp = nn.Dropout(0.4)\n","\n","        self.linear1 = nn.Linear(2048,num_classes)\n","\n","        # Pooling Layer\n","        self.avgpool = nn.AdaptiveAvgPool2d(1)\n","      \n","    # Forward pass\n","    def forward(self, x):\n","        batch_size,seq_length, c, h, w = x.shape\n","\n","        x = x.view(batch_size * seq_length, c, h, w)\n","\n","        fmap = self.model(x)\n","        \n","        x = self.avgpool(fmap)\n","\n","        x = x.view(batch_size,seq_length,2048)\n","\n","        x_lstm,_ = self.lstm(x,None)\n","        \n","        return fmap,self.dp(self.linear1(x_lstm[:,-1,:]))"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":579,"status":"ok","timestamp":1673167193619,"user":{"displayName":"I190538 Zain Ul Abidin","userId":"16664429988624671331"},"user_tz":-300},"id":"avpp16KLze7T"},"outputs":[],"source":["im_size = 112\n","mean=[0.485, 0.456, 0.406]\n","std=[0.229, 0.224, 0.225]\n","\n","sm = nn.Softmax()\n","\n","inv_normalize =  transforms.Normalize(mean=-1*np.divide(mean,std),std=np.divide([1,1,1],std))\n","def im_convert(tensor):\n","    # Display tensor as an image\n","    image = tensor.to(\"cpu\").clone().detach()\n","    image = image.squeeze()\n","    image = inv_normalize(image)\n","    image = image.numpy()\n","    image = image.transpose(1,2,0)\n","    image = image.clip(0, 1)\n","    cv2.imwrite('./2.png',image*255)\n","    return image\n","\n","def predict(model,img,path = './'):\n","  fmap,logits = model(img.to('cuda'))\n","\n","  params = list(model.parameters())\n","\n","  weight_softmax = model.linear1.weight.detach().cpu().numpy()\n","\n","  logits = sm(logits)\n","\n","  _,prediction = torch.max(logits,1)\n","\n","  confidence = logits[:,int(prediction.item())].item()*100\n","  \n","  # print(\"Confidence of prediction : \",logits[:,int(prediction.item())].item()*100)\n","\n","  idx = np.argmax(logits.detach().cpu().numpy())\n","\n","  bz, nc, h, w = fmap.shape\n","\n","  out = np.dot(fmap[-1].detach().cpu().numpy().reshape((nc, h*w)).T,weight_softmax[idx,:].T)\n","\n","  predict = out.reshape(h,w)\n","\n","  predict = predict - np.min(predict)\n","\n","  predict_img = predict / np.max(predict)\n","\n","  predict_img = np.uint8(255*predict_img)\n","\n","  out = cv2.resize(predict_img, (im_size,im_size))\n","  \n","  heatmap = cv2.applyColorMap(out, cv2.COLORMAP_JET)\n","\n","  img = im_convert(img[:,-1,:,:,:])\n","\n","  result = heatmap * 0.5 + img*0.8*255\n","\n","  cv2.imwrite('/content/1.png',result)\n","\n","  result1 = heatmap * 0.5/255 + img*0.8\n","\n","  r,g,b = cv2.split(result1)\n","\n","  result1 = cv2.merge((r,g,b))\n","\n","  # Plotting \n","  plt.imshow(result1)\n","\n","  plt.show()\n","\n","  return [int(prediction.item()),confidence]\n"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1673167199771,"user":{"displayName":"I190538 Zain Ul Abidin","userId":"16664429988624671331"},"user_tz":-300},"id":"asSbpP8fzlFj"},"outputs":[],"source":["#!pip3 install face_recognition\n","import torch\n","import torchvision\n","from torchvision import transforms\n","from torch.utils.data import DataLoader\n","from torch.utils.data.dataset import Dataset\n","import os\n","import numpy as np\n","import cv2\n","import matplotlib.pyplot as plt\n","import face_recognition\n","class validation_dataset(Dataset):\n","    def __init__(self,video_names,sequence_length = 60,transform = None):\n","        self.video_names = video_names\n","        self.transform = transform\n","        self.count = sequence_length\n","    def __len__(self):\n","        return len(self.video_names)\n","    def __getitem__(self,idx):\n","        video_path = self.video_names[idx]\n","        frames = []\n","        a = int(100/self.count)\n","\n","        first_frame = np.random.randint(0,a)     \n","         \n","        for i,frame in enumerate(self.frame_extract(video_path)):\n","            \n","            faces = face_recognition.face_locations(frame)\n","            try:\n","              top,right,bottom,left = faces[0]\n","              frame = frame[top:bottom,left:right,:]\n","            except:\n","              pass\n","            frames.append(self.transform(frame))\n","            if(len(frames) == self.count):\n","              break\n","        \n","        frames = torch.stack(frames)\n","\n","        frames = frames[:self.count]\n","\n","        return frames.unsqueeze(0)\n","\n","    # Getting frames from a video\n","    def frame_extract(self,path):\n","      vidObj = cv2.VideoCapture(path) \n","      success = 1\n","      while success:\n","          success, image = vidObj.read()\n","          if success:\n","              yield image\n","\n","def im_plot(tensor):\n","  \n","    image = tensor.cpu().numpy().transpose(1,2,0)\n","    b,g,r = cv2.split(image)\n","    image = cv2.merge((r,g,b))\n","    image = image*[0.22803, 0.22145, 0.216989] +  [0.43216, 0.394666, 0.37645]\n","    image = image*255.0\n","    plt.imshow(image.astype(int))\n","    plt.show()"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":519,"referenced_widgets":["85e0f70cd1774f418930c68795d15d71","f156acc7908145debaf57d0ef9c3e80d","a9ea4b2986fc47ccbceab98b38b16bda","a274a6d2f17040aba3e60b2b709bd53f","a00c790e7d014112bf17e502117abcb4","9357b139e5e94ff6982ea260c5dbf526","cf20dc62451f4b799b5a0a15c91f93e8","6c756b553c8d491a8782ca45d41fe448","d1b9a134e0524b489b4ebb64406986ac","a444a9de99e4415fb0b53d35943906bd","190a7519a97947eaa17a5cfe460d61d8"]},"executionInfo":{"elapsed":9105,"status":"error","timestamp":1673167213956,"user":{"displayName":"I190538 Zain Ul Abidin","userId":"16664429988624671331"},"user_tz":-300},"id":"J8YkC-vwzrkE","outputId":"43b9e458-2c72-4664-e99c-0b91bf7b585c"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNeXt50_32X4D_Weights.IMAGENET1K_V1`. You can also use `weights=ResNeXt50_32X4D_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth\" to /root/.cache/torch/hub/checkpoints/resnext50_32x4d-7cdf4587.pth\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0.00/95.8M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85e0f70cd1774f418930c68795d15d71"}},"metadata":{}},{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-c578acdcaca2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mpath_to_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/FYP/Models/Model-1.pt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 771\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    772\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/FYP/Models/Model-1.pt'"]}],"source":["#Code for making prediction\n","im_size = 112\n","mean=[0.485, 0.456, 0.406]\n","std=[0.229, 0.224, 0.225]\n","\n","train_transforms = transforms.Compose([\n","                                        transforms.ToPILImage(),\n","                                        transforms.Resize((im_size,im_size)),\n","                                        transforms.ToTensor(),\n","                                        transforms.Normalize(mean,std)])\n","\n","\n","path_to_videos= [\"/content/drive/MyDrive/FYP/Test/Fake/986_994.mp4\"]\n","\n","video_dataset = validation_dataset(path_to_videos,sequence_length = 20,transform = train_transforms)\n","\n","model = Model(2).cuda()\n","\n","path_to_model = '/content/drive/MyDrive/FYP/Models/Model-1.pt'\n","\n","model.load_state_dict(torch.load(path_to_model))\n","\n","model.eval()\n","for i in range(0,len(path_to_videos)):\n","\n","  print(path_to_videos[i])\n","\n","  prediction = predict(model,video_dataset[i],'./')\n","\n","  if prediction[0] == 1:\n","    print(\"-------------\")\n","    print(\"Video is Real\")\n","    print(\"-------------\")\n","  else:\n","    print(\"-------------\")\n","    print(\"Video is Fake\")\n","    print(\"-------------\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"ndfAa5QGP1ez"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"85e0f70cd1774f418930c68795d15d71":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f156acc7908145debaf57d0ef9c3e80d","IPY_MODEL_a9ea4b2986fc47ccbceab98b38b16bda","IPY_MODEL_a274a6d2f17040aba3e60b2b709bd53f"],"layout":"IPY_MODEL_a00c790e7d014112bf17e502117abcb4"}},"f156acc7908145debaf57d0ef9c3e80d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9357b139e5e94ff6982ea260c5dbf526","placeholder":"​","style":"IPY_MODEL_cf20dc62451f4b799b5a0a15c91f93e8","value":"100%"}},"a9ea4b2986fc47ccbceab98b38b16bda":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6c756b553c8d491a8782ca45d41fe448","max":100441675,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d1b9a134e0524b489b4ebb64406986ac","value":100441675}},"a274a6d2f17040aba3e60b2b709bd53f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a444a9de99e4415fb0b53d35943906bd","placeholder":"​","style":"IPY_MODEL_190a7519a97947eaa17a5cfe460d61d8","value":" 95.8M/95.8M [00:00&lt;00:00, 218MB/s]"}},"a00c790e7d014112bf17e502117abcb4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9357b139e5e94ff6982ea260c5dbf526":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cf20dc62451f4b799b5a0a15c91f93e8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6c756b553c8d491a8782ca45d41fe448":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d1b9a134e0524b489b4ebb64406986ac":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a444a9de99e4415fb0b53d35943906bd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"190a7519a97947eaa17a5cfe460d61d8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}